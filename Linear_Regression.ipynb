{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Linear Regression\n",
        "\n",
        "####1) What is Simple Linear Regression?\n",
        "\n",
        "Ans: Simple Linear Regression is nothing but a statistical method which is used to evaluate strength and characteristics of a linear relationship between one independent and one dependent variable.\n",
        "\n",
        "####2) What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "Ans: Key Assumptions:\n",
        "Linearity: The relationship between independent variable(X) and dependent variable(ùëå)  is linear.\n",
        "Independence: The residuals (errors) are independent.\n",
        "Homoscedasticity: The residuals have constant variance at every level of ùëã\n",
        "Normality: The residuals of the model are normally distributed.\n",
        "\n",
        "By ensuring these assumptions are met, the model's estimates will be more reliable and the predictions more accurate.\n",
        "\n",
        "####3) What does the coefficient m represent in the equation Y=mX+c?\n",
        "\n",
        "Ans: The coefficient m in the equation ùëå=ùëöùëã+ùëê  represents the slope of the line in a linear equation. It indicates how much the dependent variable\n",
        "ùëå changes for a unit increase in the independent variable\n",
        "\n",
        "####4) What does the intercept c represent in the equation Y=mX+c?\n",
        "\n",
        "Ans: The intercept c in the equation ùëå=ùëöùëã+ùëê represents the point at which the line crosses the Y-axis. It is the value of ùëå when ùëã=0. In more simpler words , Intercept C is nothing but value of target/dependent variable when the independent features have no(means x=0) effect on it.\n",
        "\n",
        "####5) How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        "Ans: m=sigma((x-xbar)(y-ybar))/(x-xbar)\n",
        "slope = model.coef_  #using sklearn\n",
        "\n",
        "####6) What is the purpose of the least squares method in Simple Linear Regression?\n",
        "\n",
        "Ans: The least squares method is essential in Simple Linear Regression because it ensures that the line of best fit minimizes the overall error between the observed data points and the predicted values. The method minimizes the sum of the squared differences (errors) between the actual values (ùëå) and the predicted values (ùëå^) from the regression line. Squaring the errors emphasizes larger errors and ensures that positive and negative.\n",
        "\n",
        "####7) How is the coefficient of determination (R¬≤) interpreted in Simple Linear Regression?\n",
        "\n",
        "Ans: R2score is an usueful evalution metric which indiates how much accurately that we can predict a dependent variable from an independent variable/feature.\n",
        "\n",
        "####8) What is Multiple Linear Regression?\n",
        "\n",
        "Ans: Multiple Linear Regression (MLR) is a statistical technique used to model the relationship between a dependent/target variable (Y) and two or more independent variables (x1,x2,x3,x4....)\n",
        "\n",
        "####9) What is the main difference between Simple and Multiple Linear Regression?\n",
        "\n",
        "Ans: In SLR we used to find the strength and charateristics of target variable from one independent variable/feature.\n",
        "In MLR we used to find the strength and charateristics of target variable from two or more independent variables/features.\n",
        "\n",
        "####10) What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "Ans: Key Assumptions of MLR:\n",
        "Linearity: The relationship between Y and each X is linear.\n",
        "No Multicollinearity: Independent variables should not be highly correlated.\n",
        "Homoscedasticity: Constant variance of errors across all values of X.\n",
        "Independence of Errors: Error terms should not be correlated.\n",
        "Normality of Errors: Residuals (errors) should be normally distributed.\n",
        "\n",
        "####11) What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "\n",
        "Ans: Heteroscedasticity occurs when the spread of residuals increases or decreases as the value of the predictor variables changes., the variance of the error terms (residuals) is not constant across all levels of the independent variables.\n",
        "The estimated standard errors of the regression coefficients become unreliable, means biased values will be generated. In simple words it leads to incorrect inferences.\n",
        "\n",
        "####12) How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "\n",
        "Ans: Multicollinearity occurs when two or more independent variables in a regression model are highly correlated. High multicollinearity inflates standard errors, leading to incorrect p-values and confidence intervals.\n",
        "If two or more independent variables are highly correlated, consider removing one of them.\n",
        "To Reduce the impact of multicollinearity by adding a penalty, also shrink some coefficients to zero.\n",
        "\n",
        "####13) What are some common techniques for transforming categorical variables for use in regression models?\n",
        "\n",
        "Ans: One-Hot Encoding (OHE),Label Encoding and binary encoding are commonly used techniques to tranform categorical variable for use in regression models. Apart from the above we can also use Ordinal Endoing and Binning as categorical variable techniques.\n",
        "\n",
        "\n",
        "####14) What is the role of interaction terms in Multiple Linear Regression?\n",
        "\n",
        "Ans:  In multiple Linear Regression, interaction plays a vital role to capture more complex relationships between variables, potentially leading to a better fit to the data and improved predictive accuracy.\n",
        "\n",
        "####15) How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "\n",
        "Ans: In both SLR and MLR the intercept represents the predicted value of the dependent variable when all independent variables are zero. The intepretation of intercept mainly differ in MLR when it considers the influence of multiple features, while in simple regression, it focuses on a single independent variable.\n",
        "\n",
        "####16) What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "\n",
        "Ans: In regression analysis, the slope signifies the change in the dependent variable for every one-unit change in the independent variable, influencing predictions by indicating the direction and magnitude of the relationship.\n",
        "In simple terms slope significantly affects the predictions by helping in finding the best fit(regression line) line.\n",
        "\n",
        "####17) How does the intercept in a regression model provide context for the relationship between variables?\n",
        "\n",
        "Ans: In a regression model, the intercept provides context for the relationship between variables by representing the predicted value of the dependent variable when all independent variables are zero, serving as a baseline or starting point for the relationship.\n",
        "In other words, Intercept is the value of dependent variable when the all the independent features are zero/constant.\n",
        "\n",
        "\n",
        "####18) What are the limitations of using R¬≤ as a sole measure of model performance?\n",
        "\n",
        "Ans: R2 Score simply denotes the model performance, r2 score ranges between 0 to 100 and a hign r2 score denotes model is performing well.\n",
        "But here, we have some limitation as well,\n",
        "It does not talk to abou whether fetures are biased or not , and also it doesn't convey the reliability of the model or whether you've chosen the right regression.\n",
        "\n",
        "####19) How would you interpret a large standard error for a regression coefficient?\n",
        "\n",
        "Ans: A large standard error for a regression coefficient suggests that the test value or original value is significantly away/differ from the predicted value.\n",
        "\n",
        "####20) How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "\n",
        "Ans: Plotting residuals against predicted values is the primary method for detecting heteroscedasticity. Residual plots are very cruicial in addressing it because it violates the assumptions of linear regression, leading to inaccurate estimates and biased results.\n",
        "\n",
        "####21) What does it mean if a Multiple Linear Regression model has a high R¬≤ but low adjusted R¬≤?\n",
        "\n",
        "Ans: A high R-squared and low adjusted R-squared in a multiple linear regression model suggests that the model's fit to the training data is good, but the model may be overfitting the data, it's capturing noise rather than true relationships so that model is performaing poor on test data.\n",
        "\n",
        "####22) Why is it important to scale variables in Multiple Linear Regression?\n",
        "\n",
        "Ans: Scaling can lead to faster convergence and better performance, particularly when using algorithms like gradient descent that are sensitive to feature scales. And it helps ensure that features with larger magnitudes don't disproportionately influence the model, leading to more accurate and interpretable results.\n",
        "\n",
        "####23) What is polynomial regression?\n",
        "\n",
        "Ans: Polynomial regression is an extension of linear regression that models the relationship between a dependent and independent variable to capture of non-linear relationships in data. When the relationship between variables isn't linear, polynomial regression can be used to fit a curve to the data, providing a better model than a simple straight line.\n",
        "\n",
        "####24) How does polynomial regression differ from linear regression?\n",
        "\n",
        "Ans: Primarily, Polynomial regression differn from liner regression when the relationship between dependent and indepent variable is non-linear. It fits the best fit line in a curved nature rather than a simple straight line.\n",
        "\n",
        "####25) When is polynomial regression used?\n",
        "\n",
        "Ans: Use polynomial regression when the relationship between variables is non-linear, and a straight line doesn't accurately capture the data.\n",
        "\n",
        "####26) What is the general equation for polynomial regression?\n",
        "\n",
        "Ans: General equation of polynomial regression: Y = a + b1X + b2X2+....bnxn+Error (where n is the degree)  \n",
        "\n",
        "\n",
        "####27) Can polynomial regression be applied to multiple variables?\n",
        "\n",
        "Ans: Yes, polynomial regression can be applied to multiple variables, also known as multivariate polynomial regression, allowing for modeling non-linear relationships between multiple input variables and a target variable.\n",
        "\n",
        "####28) What are the limitations of polynomial regression?\n",
        "\n",
        "Ans : Essentially we can say polynomial regression is prone to overfitting problem , and very sensitive to outliers.\n",
        "\n",
        "####29) What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "\n",
        "Ans: To evaluate model fit and select the appropriate degree for a polynomial, you can use methods like cross-validation and metrics like R-squared or RMSE to assess performance and prevent overfitting.\n",
        "\n",
        "\n",
        "####30) Why is visualization important in polynomial regression?\n",
        "\n",
        "Ans: Visualization is crucial in polynomial regression because it helps to understand and interpret the best fit line, identify non-linear relationships, and assess the model's performance visually, especially when comparing different polynomial degrees.\n",
        "\n",
        "####31) How is polynomial regression implemented in Python?\n",
        "\n",
        "Ans: #Example: Create quadratic features (degree=2)\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "\n",
        "X_train_poly = poly.fit_transform(X_train)\n",
        "\n",
        "X_test_poly = poly.transform(X_test)\n"
      ],
      "metadata": {
        "id": "qjv7nTp_gq7I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeDUjCn9gk_i"
      },
      "outputs": [],
      "source": []
    }
  ]
}